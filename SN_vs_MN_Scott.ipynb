{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9d3d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.model_selection import LeaveOneGroupOut, cross_val_score, cross_val_predict\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import scipy.stats as stats\n",
    "from mat_preproc import preproc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d941d500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the mat file is generated by CRM_SN_vs_MN_imbalLDA_1450ms_order_proj.m\n",
    "\n",
    "#Load in the MATLAB file with all the data\n",
    "data = scipy.io.loadmat('data_CRM_SN_vs_MN_imbalLDA_order_proj_1.mat')\n",
    "# data = scipy.io.loadmat('data_CRM_SN_vs_MN_imbalLDA_order_proj_2.mat')\n",
    "\n",
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc160e5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Had to index them like this because there was an unnecessary dimension \n",
    "\n",
    "# user trail order\n",
    "tr_order = data['user_tr_order_1'][0]\n",
    "\n",
    "# projection scores\n",
    "proj_score = data['user_prob_1'][0]\n",
    "\n",
    "# source and response label\n",
    "source_label = data['user_source_1'][0]\n",
    "resp_label = data['user_resp_1'][0]\n",
    "\n",
    "# features, group the channels and average over the windows,\n",
    "# those are called features, and we use the features for training\n",
    "behav_feat = data['user_feat_1'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4d9325",
   "metadata": {},
   "source": [
    "# Encodings for Each Label\n",
    "\n",
    "## *source information*\n",
    "\n",
    "1. SC (Source Correct)\n",
    "2. CR (Correct Rejection)\n",
    "3. SI (Source Incorrect)\n",
    "4. Miss \n",
    "5. FA  (False Alarm)\n",
    "\n",
    "## *label for the source response*\n",
    "\n",
    "1. RS (Remember Source)\n",
    "2. RO (Remember Other)\n",
    "3. F (Familiarity)\n",
    "4. MN (Maybe New) \n",
    "5. SN (Sure New)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdb6a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class attributes\n",
    "source_info = [\"SC\", \"CR\", \"SI\", \"M\", \"FA\"]\n",
    "response_info = [\"RS\", \"RO\", \"F\", \"MN\", \"SN\"]\n",
    "\n",
    "# the x-axis on the projection graph\n",
    "x_axis = [(1, 1), (3, 1), (5, 1), (1, 2), (5, 2), (1, 3), (3, 3), (5, 3), (4, 4), (2, 4), (4, 5), (2, 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c3ce70",
   "metadata": {},
   "outputs": [],
   "source": [
    "for source, resp in x_axis:\n",
    "    print(f\"{source_info[source-1]}-{response_info[resp-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46643f8",
   "metadata": {},
   "source": [
    "# Preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a8fe22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple group\n",
    "file_path = \"data_CRM_SN_vs_MN_imbalLDA_order_proj_1.mat\"\n",
    "# file_path = \"data_CRM_SN_vs_MN_imbalLDA_order_proj_2.mat\"\n",
    "data_preproc = preproc(file_path, 1)\n",
    "pos1, neg1 = data_preproc.filter_index(2,5,2,4)\n",
    "pos2, neg2 = data_preproc.filter_index(4,5,4,4)\n",
    "pos_idx, neg_idx = data_preproc.merge_two_class(pos1, neg1, pos2, neg2)\n",
    "X, y, subject = data_preproc.get_data_by_index(pos_idx, neg_idx)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37c5cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6927059d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425a94ce",
   "metadata": {},
   "source": [
    "# trainning - `CRSN` v.s. `CRMN`\n",
    "\n",
    "This has limited performance since `CRSN` and `CRMN` are only a subset of the `SN` and `MN` categories. To fully train on the `SN` and `MN` class, we need to merge `CRSN` and `MSN` and `CRMN` and `MMN` together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d825ef9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pre = preproc(file_path, 1)\n",
    "pos_idx, neg_idx = data_pre.filter_index(2,5,2,4)\n",
    "X, y, subject = data_pre.get_data_by_index(pos_idx, neg_idx)\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "print(f\"there are {logo.get_n_splits(X, y, subject)} groups\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3518df",
   "metadata": {},
   "outputs": [],
   "source": [
    "(y == 1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cfcaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA = LinearDiscriminantAnalysis(shrinkage = 0, solver = 'lsqr')\n",
    "scores = cross_val_score(LDA, X, y, cv=logo, groups=subject)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d962d4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA = LinearDiscriminantAnalysis(shrinkage = 'auto', solver = 'eigen')\n",
    "pred = cross_val_predict(LDA, X, y, cv=logo, groups=subject)\n",
    "accuracy_score(y, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bcf8b9",
   "metadata": {},
   "source": [
    "# Trainning - SN v.s. MN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc2403a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# in code, they did not leave out the two participant.\n",
    "data_preproc = preproc(file_path, 1)\n",
    "# combine SN and MN\n",
    "pos1, neg1 = data_preproc.filter_index(2,5,2,4)\n",
    "pos2, neg2 = data_preproc.filter_index(4,5,4,4)\n",
    "\n",
    "pos_idx, neg_idx = data_preproc.merge_two_class(pos1, neg1, pos2, neg2)\n",
    "X, y, subject = data_preproc.get_data_by_index(pos_idx, neg_idx, eliminate_trails=False)\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "scores = []\n",
    "\n",
    "for train_idx, test_idx in logo.split(X, y, subject):\n",
    "    X_train, y_train = X[train_idx,:], y[train_idx]\n",
    "    X_test, y_test = X[test_idx,:], y[test_idx]\n",
    "    LDA = LinearDiscriminantAnalysis(shrinkage = 0, solver = 'eigen')\n",
    "    LDA.fit(X_train, y_train)\n",
    "    scores.append(LDA.score(X_test, y_test))\n",
    "scores = np.array(scores)\n",
    "\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a7d04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9d3c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "logo.get_n_splits(groups=subject)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4aef9db",
   "metadata": {},
   "source": [
    "0.5421"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349e6250",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "LDA = LinearDiscriminantAnalysis(shrinkage = None, solver = 'lsqr')\n",
    "scores = cross_val_score(LDA, X, y, cv=logo, groups=subject)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18ed9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d46394",
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA = LinearDiscriminantAnalysis(shrinkage = \"auto\", solver = 'eigen')\n",
    "pred = cross_val_predict(LDA, X, y, cv=logo, groups=subject)\n",
    "accuracy_score(y, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe278699",
   "metadata": {},
   "source": [
    "# Testing the multiclass projection onto this classifier\n",
    "\n",
    "In Keuida's code, the projection are calculated within fold. e.g, each participant's projection is based the LDA trained on the rest of the participants' data, instead of fitting it directly. Therefore, we might need to do the manual fitting in each fold of the LOSO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dda4ced",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "projections = []\n",
    "\n",
    "curr = 0\n",
    "for train_idx, test_idx in logo.split(X, y, subject):\n",
    "    # select out the id of the participant that we left out\n",
    "    participant = int(subject[curr])\n",
    "    curr += len(test_idx)\n",
    "    # this LOSO follows the sequence of ppl presented in subject\n",
    "    X_train, X_test, y_train, y_test = X[train_idx], X[test_idx], y[train_idx], y[test_idx]\n",
    "    LDA.fit(X_train, y_train)\n",
    "    # project the whole data of this complete participant\n",
    "    X_subject = data_preproc.get_data_by_participant(participant)\n",
    "    projection = LDA.transform(X_subject)\n",
    "    projections.append(projection)\n",
    "projections = np.concatenate(projections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d534abf1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_preproc.filter_index_single_class(1,1,include_left_out=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a539bfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(projections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d085cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_name = np.array([f\"{source_info[s-1]}-{response_info[r-1]}\" for s, r in x_axis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f53ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_idx = [10, 11]\n",
    "neg_idx = [8, 9]\n",
    "\n",
    "summary_stats = []\n",
    "\n",
    "for source, response in x_axis:\n",
    "    idx = data_preproc.filter_index_single_class(source, response, include_left_out=False)\n",
    "    proj = projections[idx]\n",
    "    mean = proj.mean()\n",
    "    std = proj.std() / np.sqrt(len(proj)) * 1.96\n",
    "    summary_stats.append((mean, std))\n",
    "\n",
    "summary_stats = np.array(summary_stats)\n",
    "\n",
    "plt.errorbar(x_name, summary_stats[:,0], yerr = summary_stats[:,1],\n",
    "             fmt='o', capsize=3, c = 'black')\n",
    "# pos\n",
    "plt.errorbar(x_name[pos_idx], summary_stats[pos_idx,0],\n",
    "             yerr = summary_stats[pos_idx,1] , fmt='o', capsize=3, c = 'lime', label=\"neg class\")\n",
    "# neg\n",
    "plt.errorbar(x_name[neg_idx], summary_stats[neg_idx,0],\n",
    "             yerr = summary_stats[neg_idx,1] , fmt='o', capsize=3, c = 'red', label=\"pos class\")\n",
    "\n",
    "_ = plt.xticks(rotation=45)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72198ae1",
   "metadata": {},
   "source": [
    "# Replicate Acc based on Kueida's Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb80550",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data_CRM_SN_vs_MN_imbalLDA_order_proj_1.mat\"\n",
    "data_preproc = preproc(file_path, 1)\n",
    "# combine SN and MN\n",
    "pos1, neg1 = data_preproc.filter_index(2,5,2,4)\n",
    "pos2, neg2 = data_preproc.filter_index(4,5,4,4)\n",
    "\n",
    "pos_idx, neg_idx = data_preproc.merge_two_class(pos1, neg1, pos2, neg2)\n",
    "X, y, subject = data_preproc.get_data_by_index(pos_idx, neg_idx)\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "scores = []\n",
    "\n",
    "for train_idx, test_idx in logo.split(X, y, subject):\n",
    "    X_train, y_train = X[train_idx,:], y[train_idx]\n",
    "    X_test, y_test = X[test_idx,:], y[test_idx]\n",
    "    LDA = LinearDiscriminantAnalysis(shrinkage = None, solver = 'eigen')\n",
    "    LDA.fit(X_train, y_train)\n",
    "    # randomly drop datapoint to balance class\n",
    "    pos_idx, neg_idx = np.arange(len(test_idx))[y_test == 1], np.arange(len(test_idx))[y_test != 1]\n",
    "    pos_len, neg_len = len(pos_idx), len(neg_idx)\n",
    "    acc = []\n",
    "    for _ in range(100):\n",
    "        if pos_len > neg_len:\n",
    "            # when there are more positive class than negative\n",
    "            # randomly drop positive class to equivalent the negative class\n",
    "            pos_chosen = np.random.choice(pos_idx, neg_len, replace=False)\n",
    "            neg_chosen = neg_idx\n",
    "        else:\n",
    "            pos_chosen = pos_idx\n",
    "            neg_chosen = np.random.choice(neg_idx, pos_len, replace=False)\n",
    "        filter_test_idx = np.concatenate([pos_chosen, neg_chosen])\n",
    "        X_test_balanced, y_test_balanced = X_test[filter_test_idx, :], y_test[filter_test_idx]\n",
    "        assert sum(y_test_balanced) == 0 # to check whether they are balanced class\n",
    "        acc.append(LDA.score(X_test_balanced, y_test_balanced))\n",
    "    acc = np.array(acc)\n",
    "    scores.append(acc)\n",
    "scores = np.array(scores)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9f0780",
   "metadata": {},
   "source": [
    "Acc reported in the literature is 0.5421"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d3814c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data_CRM_SN_vs_MN_imbalLDA_order_proj_2.mat\"\n",
    "data_preproc = preproc(file_path, 2)\n",
    "# combine SN and MN\n",
    "pos1, neg1 = data_preproc.filter_index(2,5,2,4)\n",
    "pos2, neg2 = data_preproc.filter_index(4,5,4,4)\n",
    "\n",
    "pos_idx, neg_idx = data_preproc.merge_two_class(pos1, neg1, pos2, neg2)\n",
    "X, y, subject = data_preproc.get_data_by_index(pos_idx, neg_idx)\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "scores = []\n",
    "\n",
    "for train_idx, test_idx in logo.split(X, y, subject):\n",
    "    X_train, y_train = X[train_idx,:], y[train_idx]\n",
    "    X_test, y_test = X[test_idx,:], y[test_idx]\n",
    "    LDA = LinearDiscriminantAnalysis(shrinkage = \"auto\", solver = 'eigen')\n",
    "    LDA.fit(X_train, y_train)\n",
    "    # randomly drop datapoint to balance class\n",
    "    pos_idx, neg_idx = np.arange(len(test_idx))[y_test == 1], np.arange(len(test_idx))[y_test != 1]\n",
    "    pos_len, neg_len = len(pos_idx), len(neg_idx)\n",
    "    acc = []\n",
    "    for _ in range(100):\n",
    "        if pos_len > neg_len:\n",
    "            # when there are more positive class than negative\n",
    "            # randomly drop positive class to equivalent the negative class\n",
    "            pos_chosen = np.random.choice(pos_idx, neg_len, replace=False)\n",
    "            neg_chosen = neg_idx\n",
    "        else:\n",
    "            pos_chosen = pos_idx\n",
    "            neg_chosen = np.random.choice(neg_idx, pos_len, replace=False)\n",
    "        filter_test_idx = np.concatenate([pos_chosen, neg_chosen])\n",
    "        X_test_balanced, y_test_balanced = X_test[filter_test_idx, :], y_test[filter_test_idx]\n",
    "        assert sum(y_test_balanced) == 0 # to check whether they are balanced class\n",
    "        acc.append(LDA.score(X_test_balanced, y_test_balanced))\n",
    "    acc = np.array(acc)\n",
    "    scores.append(acc)\n",
    "scores = np.array(scores)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80899e20",
   "metadata": {},
   "source": [
    "acc reported in the literature is 0.5653"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0b40ee",
   "metadata": {},
   "source": [
    "# Test the AUROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306b63bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_preproc = preproc(file_path, 1)\n",
    "# combine SN and MN\n",
    "pos1, neg1 = data_preproc.filter_index(2,5,2,4)\n",
    "pos2, neg2 = data_preproc.filter_index(4,5,4,4)\n",
    "\n",
    "pos_idx, neg_idx = data_preproc.merge_two_class(pos1, neg1, pos2, neg2)\n",
    "X, y, subject = data_preproc.get_data_by_index(pos_idx, neg_idx)\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "scores = []\n",
    "\n",
    "y_pred_prob = []\n",
    "y_true = []\n",
    "\n",
    "for train_idx, test_idx in logo.split(X, y, subject):\n",
    "    X_train, y_train = X[train_idx,:], y[train_idx]\n",
    "    X_test, y_test = X[test_idx,:], y[test_idx]\n",
    "    LDA = LinearDiscriminantAnalysis(shrinkage = 'auto', solver = 'eigen')\n",
    "    LDA.fit(X_train, y_train)\n",
    "    y_pred_prob.append(LDA.predict_proba(X_test)[:,1])\n",
    "    y_true.append(y_test)\n",
    "y_pred_prob, y_true = np.concatenate(y_pred_prob), np.concatenate(y_true)\n",
    "roc_auc_score(y_true, y_pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0f5c43",
   "metadata": {},
   "source": [
    "The literature report this number as 0.5564"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8d5b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data_CRM_SN_vs_MN_imbalLDA_order_proj_2.mat\"\n",
    "\n",
    "data_preproc = preproc(file_path, 2)\n",
    "# combine SN and MN\n",
    "pos1, neg1 = data_preproc.filter_index(2,5,2,4)\n",
    "pos2, neg2 = data_preproc.filter_index(4,5,4,4)\n",
    "\n",
    "pos_idx, neg_idx = data_preproc.merge_two_class(pos1, neg1, pos2, neg2)\n",
    "X, y, subject = data_preproc.get_data_by_index(pos_idx, neg_idx)\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "scores = []\n",
    "\n",
    "y_pred_prob = []\n",
    "y_true = []\n",
    "\n",
    "for train_idx, test_idx in logo.split(X, y, subject):\n",
    "    X_train, y_train = X[train_idx,:], y[train_idx]\n",
    "    X_test, y_test = X[test_idx,:], y[test_idx]\n",
    "    LDA = LinearDiscriminantAnalysis(shrinkage = 'auto', solver = 'eigen')\n",
    "    LDA.fit(X_train, y_train)\n",
    "    y_pred_prob.append(LDA.predict_proba(X_test)[:,1])\n",
    "    y_true.append(y_test)\n",
    "y_pred_prob, y_true = np.concatenate(y_pred_prob), np.concatenate(y_true)\n",
    "roc_auc_score(y_true, y_pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e0c61c",
   "metadata": {},
   "source": [
    "The literature report this number as 0.5997"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3134dd",
   "metadata": {},
   "source": [
    "# Replicate the F vs. CR clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1af0829",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
